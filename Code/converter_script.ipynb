{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Rascunhos*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "import glob\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "#from ftplib import FTP\n",
    "\n",
    "#ftp = FTP('ftp2.mmt.se')\n",
    "\n",
    "# @hidden_cell\n",
    "#ftp.login(user='900036-JCU_Image_Data', passwd='LwARfJfM')\n",
    "\n",
    "#ftp.cwd('/Data examples/1_Visibility/')\n",
    "#ftp.retrlines('LIST')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the videos (.mp4) to images (.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_mp4_to_png(video_path, frame_path):\n",
    "  \"\"\"\n",
    "  video_path -> Path to the video to convert with the video name.\n",
    "  frame path -> Path to the folder in which frames should be saved and the video name as index to add the frame info.\n",
    "  \"\"\"\n",
    "  os.chdir('E:/')\n",
    "  vidcap = cv2.VideoCapture(video_path)\n",
    "  success,image = vidcap.read()\n",
    "  count = 0\n",
    "  while success:\n",
    "    frame_path_mod = frame_path+\"_frame%d.png\" % count\n",
    "    cv2.imwrite(frame_path_mod, image)     # save frame as JPEG file      \n",
    "    success,image = vidcap.read()\n",
    "\n",
    "    if(count % 500 == 0):\n",
    "      print(f\"Frame:{count} | Success:{success}\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(DATASET_PATH):\n",
    "    \"\"\"\n",
    "        Verifies if the path of the Dataset is created\n",
    "    \"\"\"\n",
    "    if DATASET_PATH.is_dir():\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"{DATASET_PATH} does not exist, creating one...\")\n",
    "        DATASET_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"Walks through dir_path returning its contents.\"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} frames in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all(DATASET_PATH, LIST_PATHS, RAW_DATA_PATH):\n",
    "    \"\"\"\n",
    "    Convert all videos into images.\n",
    "    \"\"\"\n",
    "    \n",
    "    check_dir(DATASET_PATH)\n",
    "\n",
    "    for i in range(len(LIST_PATHS)):\n",
    "        folders = os.path.split(LIST_PATHS[i])[0]\n",
    "        dataset_full_path_folders = DATASET_PATH / folders\n",
    "        check_dir(dataset_full_path_folders)\n",
    "\n",
    "        raw_full_path_folders = RAW_DATA_PATH + LIST_PATHS[i]\n",
    "\n",
    "        os.chdir('E:/')\n",
    "        videos = glob.glob(raw_full_path_folders+\"/*.mp4\")\n",
    "\n",
    "        for j, video in enumerate(videos):\n",
    "            video_name = dataset_full_path_folders + \"/\" + os.path.split(LIST_PATHS[i])[1] + \"_\" + str(j)\n",
    "            convert_mp4_to_png(video,video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change disk directory\n",
    "base_path = Path(\"G:/Dissertation/\")\n",
    "if(Path().cwd() != Path(r\"G:\\Dissertation\")):\n",
    "    os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data_paths\n",
    "raw_data_path = Path(\"raw_data/Data examples/\")\n",
    "raw_visibility_path = raw_data_path / Path(\"1_Visibility/\")\n",
    "raw_quality_path = raw_data_path/ Path(\"2_Quality/\")\n",
    "dataset_path = Path(\"dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths = []\n",
    "VG_path = Path(\"/1_Very_Good/Vis_Vgood_1\")\n",
    "list_paths.append(VG_path)\n",
    "G_path = Path(\"/2_Good/Vis_good_1\")\n",
    "list_paths.append(G_path)\n",
    "M_path = Path(\"/3_Moderate/Vis_Mod_1\")\n",
    "list_paths.append(M_path)\n",
    "M_path1 = Path(\"/3_Moderate/Vis_Mod_2\")\n",
    "list_paths.append(M_path1)\n",
    "P_path = Path(\"/4_Poor/Vis_Poor_1\")\n",
    "list_paths.append(P_path)\n",
    "P_path1 = Path(\"/4_Poor/Vis_Poor_2\")\n",
    "list_paths.append(P_path1)\n",
    "VP_path = Path(\"/5_Very_Poor/Vis_Vpoor_1\")\n",
    "list_paths.append(VP_path)\n",
    "\n",
    "\n",
    "#convert_all(DATASET_PATH, LIST_PATHS, RAW_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into trainning and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(dataset_path)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_data(DATA_SOURCE, DATA_TRAINING, DATA_TESTING, DATA_VALIDATION, SPLIT_SIZE):\n",
    "#     \"\"\"\n",
    "#     Split all of the data into training and testing with a split size.\n",
    "\n",
    "#     SOURCE -> Folder that has the dataset folder.\n",
    "\n",
    "#     TRAINING -> Folder where to save the training data.\n",
    "\n",
    "#     TESTING -> Folder where to save the testing data.\n",
    "    \n",
    "#     SPLIT_SIZE -> ratio of training to testing data from [trainning(0-1),validation(0-1),testing(0-1)] make sure that the sum is 1.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if(sum(SPLIT_SIZE) != 1):\n",
    "#         print(\"SPLIT_SIZE is not valid\")\n",
    "#         return\n",
    "    \n",
    "#     check_dir(Path(DATA_TRAINING))\n",
    "#     check_dir(Path(DATA_TESTING))\n",
    "#     check_dir(Path(DATA_VALIDATION))\n",
    "\n",
    "#     for dir in os.listdir(DATA_SOURCE):\n",
    "#             TRAINING = DATA_TRAINING / dir\n",
    "#             TESTING = DATA_TESTING / dir\n",
    "#             VALIDATION = DATA_VALIDATION / dir\n",
    "#             check_dir(Path(TRAINING))\n",
    "#             check_dir(Path(TESTING))\n",
    "#             check_dir(Path(VALIDATION))\n",
    "\n",
    "#             SOURCE = DATA_SOURCE / dir\n",
    "\n",
    "#             files = []\n",
    "#             print('Split Data')\n",
    "#             for filename in os.listdir(SOURCE):\n",
    "#                 file = SOURCE / filename\n",
    "#                 if os.path.getsize(file) > 0:\n",
    "#                     files.append(filename)\n",
    "#                 else:\n",
    "#                     print(filename + \"is zero lenght, so ignoring.\")\n",
    "            \n",
    "#             training_length = int(len(files)* SPLIT_SIZE[0])\n",
    "#             validation_length = int(len(files)* SPLIT_SIZE[1])\n",
    "#             testing_length = int(len(files)* SPLIT_SIZE[2])\n",
    "\n",
    "#             print('SOURCE: ',SOURCE, '\\n TRAINING', TRAINING, '\\n ',len(files))\n",
    "#             print('training_length:',training_length)\n",
    "#             print('validation_length:',validation_length)\n",
    "#             print('testing_length:',testing_length)\n",
    "\n",
    "#             shuffled_set = random.sample(files, len(files))\n",
    "#             training_set = shuffled_set[0:training_length]\n",
    "#             validation_set = shuffled_set[training_length:training_length+validation_length]\n",
    "#             testing_set= shuffled_set[training_length+validation_length:]\n",
    "            \n",
    "#             for filename in training_set:\n",
    "#                 this_file = SOURCE / filename\n",
    "#                 destination = TRAINING / filename\n",
    "#                 copyfile(this_file, destination)\n",
    "\n",
    "#             for filename in validation_set:\n",
    "#                 this_file = SOURCE / filename\n",
    "#                 destination = VALIDATION / filename\n",
    "#                 copyfile(this_file, destination)\n",
    "            \n",
    "#             for filename in testing_set:\n",
    "#                 this_file = SOURCE / filename\n",
    "#                 destination = TESTING / filename\n",
    "#                 copyfile(this_file, destination)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir =  Path(\"dataset_split_1/test/\")\n",
    "validation_dir =  Path(\"dataset_split_1/validation/\")\n",
    "train_dir = Path(\"dataset_split_1/train/\")\n",
    "\n",
    "SPLIT_SIZE = [0.8,0.1,0.1]\n",
    "\n",
    "#split_data(dataset_path,train_dir,test_dir,validation_dir,SPLIT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split_path1 = Path(\"dataset_split_1/\")\n",
    "data_dir = Path(dataset_split_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "from PIL import Image\n",
    "\n",
    "# Set seed\n",
    "#random.seed(42)\n",
    "\n",
    "# 1. Get all image paths \n",
    "data_path_list = list(dataset_split_path1.glob(\"*/*/*.png\"))\n",
    "\n",
    "# 2. Pick a random image path\n",
    "random_image_path = random.choice(data_path_list)\n",
    "\n",
    "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "# 4. Open image\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# Turn the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# Plot the image with matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} \")\n",
    "plt.axis(False);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths: list, transform, n=3, seed=None):\n",
    "  \"\"\"\n",
    "  Selects random images from a path of images and loads/transforms \n",
    "  them then plots the original vs the transformed version.\n",
    "  \"\"\"\n",
    "  if seed:\n",
    "    random.seed(seed)\n",
    "  random_image_paths = random.sample(image_paths, k=n)\n",
    "  for image_path in random_image_paths:\n",
    "    with Image.open(image_path) as f:\n",
    "      fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "      ax[0].imshow(f)\n",
    "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
    "      ax[0].axis(False)\n",
    "\n",
    "      # Transform and plot target image\n",
    "      transformed_image = transform(f).permute(1, 2, 0) # note we will need to change shape for matplotlib (C, H, W) -> (H, W, C)\n",
    "      ax[1].imshow(transformed_image)\n",
    "      ax[1].set_title(f\"Transformed\\nShape: {transformed_image.shape}\")\n",
    "      ax[1].axis(\"off\")\n",
    "\n",
    "      fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a transform for image\n",
    "data_transform = transforms.Compose([\n",
    "  # Resize our images to 64x64\n",
    "  transforms.Resize(size=(224, 224)),\n",
    "  # Flip the images randomly on the horizontal and vertical\n",
    "  transforms.RandomHorizontalFlip(p=0.5),\n",
    "  transforms.RandomVerticalFlip(p=0.5),\n",
    "  # Turn the image into a torch.Tensor\n",
    "  transforms.ToTensor() \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transformed_images(image_paths=data_path_list,\n",
    "                        transform=data_transform,\n",
    "                        n=1,\n",
    "                        seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "\n",
    "#Get classes\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "  \"\"\"Finds the class folder names in a target directory.\"\"\"\n",
    "  # 1. Get the class names by scanning the target directory\n",
    "  classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "  # 2. Raise an error if class names could not be found\n",
    "  if not classes:\n",
    "    raise FileNotFoundError(f\"Couldn't find any classes in {directory}... please check file structure.\")\n",
    "\n",
    "  # 3. Create a dictionary of index labels (computers prefer numbers rather than strings as labels)\n",
    "  class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
    "  return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pathlib\n",
    "\n",
    "# Create custom dataset class\n",
    "class ImageFolderCustom(Dataset):\n",
    "  def __init__(self, \n",
    "               targ_dir: str, \n",
    "               transform=None):\n",
    "\n",
    "    self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.png\"))\n",
    "\n",
    "    self.transform = transform\n",
    "\n",
    "    self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "\n",
    "  # 4. Create a function to load images\n",
    "  def load_image(self, index: int) -> Image.Image:\n",
    "    \"Opens an image via a path and returns it.\"\n",
    "    image_path = self.paths[index]\n",
    "    return Image.open(image_path)\n",
    "\n",
    "  # 5. Overwrite __len__()\n",
    "  def __len__(self) -> int:\n",
    "    \"Returns the total number of samples.\"\n",
    "    return len(self.paths)\n",
    "  \n",
    "  # 6. Overwrite __getitem__() method to return a particular sample\n",
    "  def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "    \"Returns one sample of data, data and label (X, y).\"\n",
    "    img = self.load_image(index)\n",
    "    class_name = self.paths[index].parent.name \n",
    "    class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "    # Transform if necessary\n",
    "    if self.transform:\n",
    "      return self.transform(img), class_idx # return data, label (X, y)\n",
    "    else:\n",
    "      return img, class_idx # return untransformed image and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transform\n",
    "from torchvision import transforms\n",
    "train_transforms = transforms.Compose([\n",
    "                                      transforms.Resize(size=(224, 224)),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.RandomVerticalFlip(p=0.5),\n",
    "                                      transforms.ToTensor() \n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                      transforms.Resize(size=(224, 224)),\n",
    "                                      transforms.ToTensor()\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "                                      transforms.Resize(size=(224, 224)),\n",
    "                                      transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out ImageFolderCustom\n",
    "train_data_custom = ImageFolderCustom(targ_dir=train_dir,\n",
    "                                      transform=train_transforms)\n",
    "\n",
    "test_data_custom = ImageFolderCustom(targ_dir=test_dir,\n",
    "                                     transform=test_transforms)\n",
    "\n",
    "validation_data_custom = ImageFolderCustom(targ_dir=validation_dir,\n",
    "                                           transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageFolder to create dataset(s)\n",
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                  transform=data_transform, # a transform for the data\n",
    "                                  target_transform=None) # a transform for the label/target \n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                 transform=data_transform)\n",
    "\n",
    "validation_data = datasets.ImageFolder(root=validation_dir,\n",
    "                                 transform=data_transform)\n",
    "\n",
    "\n",
    "# Turn train and test datasets into DataLoader's\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE=32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=False)\n",
    "\n",
    "validation_dataloader = DataLoader(dataset=validation_data,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    num_workers=NUM_WORKERS,\n",
    "                                    shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My implementation\n",
    "import torch.nn as nn\n",
    "\n",
    "#Create ResNet50 Model Class\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.expansion = 4\n",
    "\n",
    "        self.sub_Block1= nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.sub_Block2= nn.Sequential(\n",
    "            nn.Conv2d(out_channels,out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.sub_Block3= nn.Sequential(\n",
    "            nn.Conv2d(out_channels,out_channels*4, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels*4)\n",
    "        )\n",
    "        \n",
    "        # self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        # self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        # self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        # self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self,x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.sub_Block1(x)\n",
    "        x = self.sub_Block2(x)\n",
    "        x = self.sub_Block3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        \n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module): # [3, 4, 6, 3]\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.initBlock = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2,padding=1), \n",
    "        )\n",
    "\n",
    "        #Resnet Layers\n",
    "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride = 1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride = 2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride = 2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride = 2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512*4, num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.initBlock(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_blocks, out_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers=[]\n",
    "        if stride != 1 or self.in_channels != out_channels * 4:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1,stride=stride),\n",
    "                                                nn.BatchNorm2d(out_channels*4))\n",
    "\n",
    "        layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
    "        self.in_channels = out_channels*4 \n",
    "\n",
    "        for i in range(num_blocks -1):\n",
    "            layers.append(ResBlock(self.in_channels, out_channels)) \n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "def ResNet50(img_channels=3, num_classes=5):\n",
    "    return ResNet(ResBlock, [3, 4, 6, 3], img_channels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50().to(device)\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single image batch\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a forward pass\n",
    "model(image_batch.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train step\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer:torch.optim.Optimizer,\n",
    "               device=device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0,0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "    \n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader) \n",
    "    return train_loss, train_acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation step\n",
    "def validation_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device=device):\n",
    "  # Put model in eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # Setup test loss and test accuracy values\n",
    "  validation_loss, validation_acc = 0,  0\n",
    "\n",
    "  # Turn on inference mode\n",
    "  with torch.inference_mode():\n",
    "    # Loop through DataLoader batches\n",
    "    for batch, (X, y) in enumerate(dataloader): \n",
    "      # Send data to the target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 1. Forward pass\n",
    "      validation_pred_logits = model(X)\n",
    "\n",
    "      # 2. Calculate the loss\n",
    "      loss = loss_fn(validation_pred_logits, y)\n",
    "      validation_loss += loss.item()\n",
    "\n",
    "      # Calculate the accuracy\n",
    "      validation_pred_labels = validation_pred_logits.argmax(dim=1)\n",
    "      validation_acc += ((validation_pred_labels == y).sum().item()/len(validation_pred_labels))\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch\n",
    "  validation_loss = validation_loss / len(dataloader)\n",
    "  validation_acc = validation_acc / len(dataloader)\n",
    "  return validation_loss, validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Create a train function that takes in various model parameters + optimizer + dataloaders + loss function\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader,\n",
    "          validation_dataloader,\n",
    "          optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5, \n",
    "          device=device):\n",
    "  \n",
    "  # 2. Create empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "             \"train_acc\": [],\n",
    "             \"validation_loss\": [],\n",
    "             \"validation_acc\": []}\n",
    "  \n",
    "  # 3. Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    train_loss, train_acc = train_step(model=model,\n",
    "                                       dataloader=train_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer,\n",
    "                                       device=device)\n",
    "    validation_loss, validation_acc = validation_step(model=model,\n",
    "                                    dataloader=validation_dataloader,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    device=device)\n",
    "    \n",
    "    # 4. Print out what's happening\n",
    "    print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {validation_loss:.4f} | \"\n",
    "          f\"test_acc: {validation_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    # 5. Update results dictionary\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"validation_loss\"].append(validation_loss)\n",
    "    results[\"validation_acc\"].append(validation_acc)\n",
    "  \n",
    "  # 6. Return the filled results at the end of the epochs\n",
    "  return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initial tests**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and define some paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "import glob\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change disk directory\n",
    "base_path = Path(\"G:/Dissertation/\")\n",
    "if(Path().cwd() != Path(r\"G:\\Dissertation\")):\n",
    "    os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define data_paths\n",
    "raw_data_path = Path(\"raw_data/Data examples/\")\n",
    "raw_visibility_path = raw_data_path / Path(\"1_Visibility/\")\n",
    "raw_quality_path = raw_data_path/ Path(\"2_Quality/\")\n",
    "dataset_path = Path('dataset/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import split_data\n",
    "\n",
    "data_dir = Path(\"small_split/\")\n",
    "split_size = [0.8,0.1,0.1]\n",
    "\n",
    "#split_data(dataset_path, data_dir, split_size, num_img_class=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 15 18:26:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.01       Driver Version: 516.01       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   61C    P8     5W /  N/A |    141MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "#Create transform (in this case for the ResNet images are resized to 224x224 and transformed into Tensors)\n",
    "data_transform = transforms.Compose([\n",
    "  # Resize our images to 64x64\n",
    "  transforms.Resize(size=(224, 224)),\n",
    "  # Flip the images randomly on the horizontal (just to make it as independent from the position of the pipeline)\n",
    "  transforms.RandomHorizontalFlip(p=0.5),\n",
    "  transforms.RandomVerticalFlip(p=0.5),\n",
    "  # Turn the image into a torch.Tensor\n",
    "  transforms.ToTensor() \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import create_dataloaders\n",
    "\n",
    "#Paths criados pelo split_data\n",
    "train_dir = data_dir / Path('train/')\n",
    "validation_dir = data_dir / Path('validation/')\n",
    "test_dir = data_dir / Path('test/')\n",
    "\n",
    "BATCH_SIZE = 32 # in the ResNet is 128 but my GPU doesn't have enough memory for that\n",
    "\n",
    "train_dataloader, validation_dataloader, test_dataloader, class_names = create_dataloaders(train_dir,test_dir,validation_dir,data_transform, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]), torch.Size([32]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a single image batch\n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2665,  0.9294,  0.3030, -0.2930,  0.1531],\n",
       "        [-2.1681,  1.8981,  0.6181, -0.6431,  0.7084],\n",
       "        [-1.1844,  0.7552,  0.2697, -0.3296,  0.1266],\n",
       "        [-1.6408,  1.0427,  0.4422, -0.5547,  0.2179],\n",
       "        [-1.3502,  1.1858,  0.1328, -0.3273,  0.2560],\n",
       "        [-1.3262,  1.0987,  0.2960, -0.4869,  0.2801],\n",
       "        [-2.4234,  1.8316,  0.5082, -0.5052,  0.6139],\n",
       "        [-2.0122,  2.1723,  0.6389, -0.8306,  0.7335],\n",
       "        [-1.2005,  0.8255,  0.2015, -0.2013,  0.0758],\n",
       "        [-1.9616,  1.5843,  0.7083, -0.3405,  0.4840],\n",
       "        [-1.2147,  0.8599,  0.2673, -0.3496,  0.2651],\n",
       "        [-1.3637,  1.0954,  0.3708, -0.2421,  0.1262],\n",
       "        [-1.3298,  0.7925,  0.2815, -0.1426,  0.1095],\n",
       "        [-1.2784,  1.1206,  0.3948, -0.4074,  0.2357],\n",
       "        [-1.1769,  0.9924,  0.3296, -0.2962,  0.3126],\n",
       "        [-1.4106,  1.1242,  0.2358, -0.2065,  0.2267],\n",
       "        [-0.9863,  0.7855,  0.1912, -0.2500,  0.2855],\n",
       "        [-1.5357,  1.3845,  0.1890, -0.3215,  0.3884],\n",
       "        [-1.2746,  0.7662,  0.3206, -0.2367,  0.1189],\n",
       "        [-1.1496,  0.8226,  0.1274, -0.1677,  0.1450],\n",
       "        [-1.2406,  0.8729,  0.2842, -0.2009, -0.0226],\n",
       "        [-1.3113,  0.7785,  0.3309, -0.1383,  0.1447],\n",
       "        [-1.2969,  1.1327,  0.2068, -0.2169,  0.3530],\n",
       "        [-1.9085,  1.9100,  0.6333, -0.5453,  0.6771],\n",
       "        [-1.2019,  0.6815,  0.2607, -0.2014,  0.0958],\n",
       "        [-1.2454,  0.8255,  0.3817, -0.2748,  0.1078],\n",
       "        [-1.3099,  0.8425,  0.2661, -0.2453,  0.0574],\n",
       "        [-1.5948,  1.2864,  0.2398, -0.4936,  0.4355],\n",
       "        [-1.8819,  1.7316,  0.5080, -0.6597,  0.6145],\n",
       "        [-1.0333,  0.7210,  0.2616, -0.2653,  0.2743],\n",
       "        [-1.5555,  1.3056,  0.2258, -0.1960,  0.4311],\n",
       "        [-2.0474,  2.0587,  0.7768, -0.6394,  0.8285]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a forward pass\n",
    "model = ResNet50().to(device)\n",
    "model(image_batch.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12676\\3633990174.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m model_results = train(model=model,\n\u001b[0m\u001b[0;32m     22\u001b[0m                     \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\joaor\\Desktop\\AQAUIS\\AQAUIS\\Code\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, validation_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[0;32m     90\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     train_loss, train_acc = train_step(model=model,\n\u001b[1;32m---> 92\u001b[1;33m                                        \u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                                        \u001b[0mloss_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                                        \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "from engine import train\n",
    "from ResNet50 import ResNet50\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "model = ResNet50().to(device)\n",
    "\n",
    "# Setup loss function and optimizer \n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer() \n",
    "\n",
    "# Train model\n",
    "model_results = train(model=model,\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    validation_dataloader=validation_dataloader,\n",
    "                    optimizer=optimizer,\n",
    "                    loss_fn=loss_fn,\n",
    "                    epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15b211d7ad94b52292045efb4f8f9084eab8b035832c108b93ce5f33d27f5980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
